#LANDING PAGE
home.subtitle=Explore the features of machine learning by using our easy to use interface.
home.title=Klessify
explorer.info=Explore weka and it's features.
information=What is machine learning?
button=Go to page

#CONTACT PAGE
contact.title=Contact
contact.subtitle=Meet the team behind our Weka Interface.
marijke.name=Marijke Eggink
jelle.name=Jelle Becirspahic
michiel.name=Michiel Noback
bart.name=Bart Engels
jelle.email=j.d.becirspahic@st.hanze.nl
marijke.email=m.eggink@st.hanze.nl
bart.email=b.engels@st.hanze.nl
michiel.email=m.a.noback@pl.hanze.nl
marijke.info=The Back End developer, responsible for all the moving parts on the server side that communicates with the HTML pages.
bart.info=Our gitmaster, responsible for version management, making \
sure that working in the same code project runs smoothly.
jelle.info=The Front End developer, creating the HTML pages that you see and\
small functionality on the pages.
michiel.info=The project supervisor, giving advice on how to tackle problems \
and providing sources of information.

#ABOUT PAGE
about.title=About
about.subtitle=What's the point of this web application?
about.text=When starting out, learning about machine learling can be incredibly \
challenging. Being able to interpretate the results of creating \
a model requires significant knowledge about algorithms, algorithm \
parameters and mathematics. Besides that, the resources that are \
available during this steep learning curve aren't very beginner friendly. \
The weka software, however powerfull, is hard to use as a beginner \
due to it's unintuitive design and lack of information.<br> \
<br> \
To alleviate this beginners burden, we have made this web \
application to provide an easy workflow combined with easy \
access to information that is required to interpretate the \
results of your models and analysis.

#INFORMATION PAGE
wekaAlgorithms.title=Weka Algortihms
WekaAlgorithms.subtitle=Open the dropdown menu's to learn more about each machine\
learning algorithm that we've implemented in this web application.
information.title=Machine Learning
information.subtitle=Merriam Webster defines machine learning as follows:<br>\
<i>"The term machine learning (abbreviated ML) refers to the \
capability of a machine to improve its own performance. \
It does so by using a statistical model to make decisions \
and incorporating the result of each new trial into that \
model. In essence, the machine is programmed to learn \
through trial and error."</i><br>
ZeroR.information=The simplest of the rule based classifiers is the majority class classifier, called 0-R or\
ZeroR in Weka. The 0-R (zero rule) classifier takes a look at the target attribute and its possible\
values. It will always output the value that is most commonly found for the target attribute in the\
given dataset. 0-R as its names suggests; it does not include any rule that works on the non target\
attributes. So more specifically it predicts the mean (for a numeric type target attribute) or the mode\
(for a nominal type attribute).
ZeroR.information.example = Example :"Play Golf = Yes" is the ZeroR model for the dataset with an accuracy of 0.64 is 9 yes and 5 no .
ZeroR.information.modelEvaluation = ZeroR only predicts the majority class correctly. As mentioned before, \
ZeroR is only useful for determining a baseline performance for other classification methods.
OneR.information=OneR, short for "One Rule", is a simple, yet accurate, classification algorithm that generates one rule \
  for each predictor in the data, then selects the rule with the smallest total error as its "one rule".  \
  To create a rule for a predictor, we construct a frequency table for each predictor against the target. \
  It has been shown that OneR produces rules only slightly less accurate than state-of-the-art classification algorithms \
  while producing rules that are simple for humans to interpret.\
  For each predictor,\
  For each value of that predictor, make a rule as follows;  \
1: Count how often each value of target (class) appears.  \
2: Find the most frequent class.   \
3: Make the rule assign that class to this value of the predictor.  \
4: Calculate the total error of the rules of each predictor.  \
5: Choose the predictor with the smallest total error.
OneR.information.example=  Example: Finding the best predictor with the smallest total error using OneR algorithm based on related frequency tables. 
OneR.information.modelEvaluation = Simply, the total error calculated from the frequency tables is the measure of \
  each predictor contribution. A low total error means a higher contribution to the predictability of the model.  
NaiveBayes.information=The Naive Bayesian classifier is based on Bayesâ€™ theorem with the \
  independence assumptions between predictors. A Naive Bayesian model is easy to build, with no complicated \
  iterative parameter estimation which makes it particularly useful for very large datasets. Despite its simplicity, \
  the Naive Bayesian classifier often does surprisingly well and is widely used because it often outperforms more sophisticated \
  classification methods. Bayes theorem provides a way of calculating the posterior probability, \
  P(c|x), from P(c), P(x), and P(x|c). Naive Bayes classifier assume that the effect of the value of a predictor (x) \
  on a given class (c) is independent \
  of the values of other predictors. This assumption is called class conditional independence.
NaiveBayes.information.example = In ZeroR model there is no predictor, in OneR model we try to find the single best predictor, naive Bayesian includes all predictors using Bayes' rule and the independence assumptions between predictors.
NaiveBayes.information.modelEvaluation = The posterior probability can be calculated by first, \
  constructing a frequency table for each attribute against the target. \
  Then, transforming the frequency tables to likelihood tables and finally use the Naive Bayesian \
  equation to calculate the posterior probability for each class. \
  The class with the highest posterior probability is the outcome of prediction. 
IBK.information=K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). \
  In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric classification\
  \ method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification \
  and regression. In both cases, the input consists of the k closest training examples in data set. The output depends on whether k-NN is\
  \ used for classification or regression:\
In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors,\
  \ with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). \
  A case is classified by a majority vote of its neighbors, with the case being assigned to the class most common amongst its K nearest neighbors measured by a distance function. \
  If K = 1, then the case is simply assigned to the class of its nearest neighbor. 	
IBK.information.example=It should also be noted that all three distance measures are only valid for continuous variables. \
  In the instance of categorical variables the Hamming distance must be used. It also brings up the issue of standardization of \
  the numerical variables between 0 and 1 when there is a mixture of numerical and categorical variables in the dataset. \
  Choosing the optimal value for K is best done by first inspecting the data. In general,\
  a large K value is more precise as it reduces the overall noise but there is no guarantee. \
  Cross-validation is another way to retrospectively determine a good K value by using an independent dataset to validate the K value. \
  Historically, the optimal K for most datasets has been between 3-10. That produces much better results than 1NN.
IBK.information.modelEvaluation= One major drawback in calculating distance measures directly from the training set is in the case where variables have different \
  measurement scales or there is a mixture of numerical and categorical variables. \
  For example, if one variable is based on annual income in dollars, and the other is based on age \
  in years then income will have a much higher influence on the distance calculated. One solution is to standardize the training set as shown below.
J48.information=J4.8 is Weka implementation of decision tree algorithm C4.5. It is actually built upon ID3\
algorithm by adding features like pruning as well as allowing numeric attribute values.\
C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. \
  C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for \
  classification, and for this reason, C4.5 is often referred to as a statistical classifier. \
  In 2011, authors of the Weka machine learning software described the C4.5 algorithm as "a landmark decision tree program \
  that is probably the machine learning workhorse most widely used in practice to date. Decision tree builds classification or regression models in the form of a tree structure. \
  It breaks down a dataset into smaller and smaller \
  subsets while at the same time an associated decision tree is incrementally developed. \
  The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). \
  Leaf node (e.g., Play) represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. \
  Decision trees can handle both categorical and numerical data.
J48.information.example = The core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, \
  greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a \
  decision tree. In ZeroR model there is no predictor, in OneR model we try to find the single best predictor, naive Bayesian includes \
  all predictors using Bayes' rule and the independence assumptions between predictors but decision tree includes all predictors with the\
  \ dependence assumptions between predictors.
J48.information.modelEvaluation = A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. \
  If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.

#NAVBAR FRAGMENT
navbar.about=About
navbar.contact=Contact
navbar.home=Home

#PARAMETER FRAGMENT
max.batchsize=Maximum Batchsize
debug.par=Debug
doNotCheck.par=Do not check capabilities
num.decimal.par=Number of decimal places
minBucketSize.par=Minimum bucketsize
confidenceFactor.par=Confidence factor
minNumObj.par=Minimum number of objects
numFold.par=Number of folds
laPlace.par=la Place estimator
pruned.par=Pruned
knn.par=K-nearest neighbour
crossValidate.par=Crossvalidate
nnsearchAlgorithm.par=Nearest neighbour \
search algorithm
true=Yes
false=No

#PARAMETER EXPLAINATION
batchsize.explaination=The preferred number of instances to process\n\
if batch prediction is being performed.
debug.explaination=If set to yes, the classifier \n\
may output additional information.
donotcheckcap.explaination=If set, classifier cpapabilities ar\n\
not checked before classifier is built.\n\
This might increase the time it takes\n\
to build the classifier.
numdecimals.explaination=The number of decimal places \n\
to be used for the output of\n\
numbers in the model.
minbucket.explaination=The minimum bucket size used\n\
for discretizing numeric attributes\n\
into groups.
confactor.explaination=The confidence factor is used in pruning.\n\
The smaller the value the more pruning is incured.
minnumobj.explaination=The minimum number of instances\n\
per leaf of the tree.
numfolds.explaination=Determines the amount of data\n\
used for 'reduced-error' pruning.
pruned.explaination=Wether the resulting tree will be pruned
laplace.explaination=
knn.explaination=The number of neighbours to use.
nnsearch.explaination=The nearest neighbour search algorithm to use
crossval.explaination=Wether hold-one-out cross-validation\n\
is applied to select the best k value\n\
between 1 and the value specified as\n\
the KNN parameter.

#FILE SELECT FORM
form.file.select=Select...
form.select=Select the demo file: 
form.upload=Or upload your own file:
form.choose=Choose file

#EXPLORER TAB
explorer=Explorer
explorer.attributes=Explorer atrributes:
explorer.select = Select attribute

#CLASSIFIER TAB
classify=Classifier
form.classifier.select=Select the classifier:

#RESULTS TAB
results=Results
results.title=Results
results.correct=Correctly Classified Instances              
results.perc.correct=Percentage Correctly Classified Instances   
results.incorrect=Incorrectly Classified Instances            
results.perc.incorrect=Percentage Incorrectly Classified Instances 
results.kappa=Kappa statistic                             
results.mean.abs=Mean absolute error                         
results.mean.squared=Root mean squared error                     
results.relative.abs=Relative absolute error                     
results.relative.squared=Root relative squared error                 
results.instances.num=Total Number of Instances                   
results.message=There are no results yet.

#HISTORY TAB
history = History
history.time = Time/Date
history.usedFile = Used file
history.uploaded=Uploaded files
history.explorer=History Explorer
history.classifier=History classifier

#RESULTS EXPLAINATION
numInstances.explaination=The number of instances used for building the model
correct.explaination=The number of correctly classified instances
pctCorrect.explaination=The percentage of the total amount of instances that have\
been classified correctly
incorrect.explaination=The amount of incorrectly classified instances
pctIncorrect.explaination=The percentage of the total amount of instances that have\
been classified incorrectly
kappa.explaination=The kappa statistic is a way of determining the reliability\
of a model. It is exressed as a value between 0 and 1. A value\
of 0 meaning it's not reliable and most likely happened by chance\
 and 1 meaning it's very reliable and probably isn't coincidental. 
meanAbsoluteError=In statistics, mean absolute error is a measure of difference between two continuous variables. Assume X and Y are variables of paired observations that express the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement.
rootMeanSquaredError=The root-mean-square deviation or root-mean-square error is a frequently used measure of the differences between values predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences.
relativeAbsoluteError=Measure the performance of a predictive model.
rootRelativeSquaredError=Relative to what it would be like if a\
   simple predictor was used.

#ERROR PAGE
404.title=Oops, this page doesn't exist
404.subtitle=The page that has been requested doesn't seem to exist.
500.title=Oops, something went wrong
500.subtitle=Something went wrong inside our server.
400.title=Oh no! A bad Request.
400.subtitle=Something went wrong communicating your input to the server.\
<br>Make sure you fill out forms before you press the 'submit' button.
