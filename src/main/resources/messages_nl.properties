#LANDING PAGE
home.subtitle=Ontdek machine learning en pas het toe met onze makkelijk te gebruiken webapplicatie.
home.title=Klessify
explorer.info=Ontdek hoe weka werkt.
button=Ga naar de pagina
information=Wat is machine learning?

#CONTACT PAGE
contact.title=Contact
contact.subtitle=Dit is het team achter onze Weka Interface.
marijke.name=Marijke Eggink
jelle.name=Jelle Becirspahic
michiel.name=Michiel Noback
bart.name=Bart Engels
jelle.email=j.d.becirspahic@st.hanze.nl
marijke.email=m.eggink@st.hanze.nl
bart.email=b.engels@st.hanze.nl
michiel.email=m.a.noback@pl.hanze.nl
marijke.info=De Back End ontwikkelaar, verantwoordelijk voor alle bewegende onderdelen achter de schermen wat communiceert met de HTML pagina's.
bart.info=Onze gitmaster, verantwoordelijk voor het versie management \
zodat wanneer je samen aan een code project werkt, alles\
soepel verloopt.
jelle.info=De Front End ontwikkelaar, maakt de HTML paginas die je ziet\
 en voegt hier fucntionaliteit aan toe.
michiel.info=De project begeleider, geeft advies over het oplossen van problemen\
 en voorziet het team van informatie bronnen.

#ABOUT PAGE
about.title=Over
about.subtitle=Wat is het nut van deze web applicatie?
about.text=Als beginner, kan leren over machine learning ongelooflijk \
uitdagend zijn.Het interpreteren van de resultaten van een \
 model vereist aanzienlijke kennis over algoritmen, algoritmen \
parameters en wiskunde. Daarnaast zijn de middelen die \
beschikbaar tijdens deze steile leercurve niet erg \
beginnersvriendelijk. De weka-software, hoe krachtig deze \
ook is, is moeilijk te gebruiken als beginner vanwege \
het niet-intuïtieve ontwerp en het gebrek aan informatie.<br> \
<br> \
Om deze beginnerslast te verlichten, hebben we deze web applicatie\
gemaakt om een gemakkelijke workflow te \
bieden in combinatie met toegang tot \
informatie die nodig is om de \
resultaten van je modellen en analyse te interpreteren.

#INFORMATION PAGE
information.title=Machine Learning
information.subtitle=Merriam Webster definieert machine learning als volgt:<br>\
<i>"De term machine learning (afgekort ML) verwijst naar het \
vermogen van een machine om zijn eigen prestaties te \
verbeteren. Het doet dit door een statistisch model te \
gebruiken om beslissingen te nemen en het resultaat van \
elke nieuwe proef in dat model op te nemen. In wezen is de \
machine geprogrammeerd om met 'trial and error' te leren."</i>
wekaAlgorithms.title=Weka Algorithme
WekaAlgorithms.subtitle=Open het dropdown menu om meer te leren over de verschillende\
machine learning algoritmen die wij geimplementeerd hebben in\
onze web applicatie.
ZeroR.information=De eenvoudigste van de op regels gebaseerde classificaties is de meerderheidsclassificatie, genaamd 0-R of\
ZeroR in Weka. De classifier 0-R (nulregel) kijkt naar het doelkenmerk en de mogelijke ervan\
waarden. Het zal altijd de waarde uitvoeren die het meest wordt gevonden voor het doelkenmerk in de \
gegeven dataset. 0-R zoals de naam doet vermoeden; het bevat geen enkele regel die werkt op het niet-doel\
attributen. Dus meer specifiek voorspelt het het gemiddelde (voor een numeriek type doelkenmerk) of de modus\
(voor een nominaal type attribuut).
ZeroR.information.example = Voorbeeld: "Play Golf = Yes" is het ZeroR-model voor de dataset met een nauwkeurigheid van 0,64 is 9 ja en 5 nee .
ZeroR.information.modelEvaluation = ZeroR voorspelt alleen de meerderheidsklasse correct. Zoals eerder vermeld, \
ZeroR is alleen nuttig voor het bepalen van een basisprestatie voor andere classificatiemethoden.
OneR.information=OneR, een afkorting voor "One Rule", is een eenvoudig, maar nauwkeurig classificatie-algoritme dat één regel genereert \
  voor elke voorspeller in de gegevens, selecteert vervolgens de regel met de kleinste totale fout als zijn "ene regel". \
  Om een regel voor een voorspeller te maken, construeren we een frequentietabel voor elke voorspeller tegen het doel. \
  Het is aangetoond dat OneR regels slechts iets minder nauwkeurig produceert dan state-of-the-art classificatie-algoritmen \
  terwijl ze regels produceren die voor mensen eenvoudig te interpreteren zijn.\
  Voor elke voorspeller,\
  Maak voor elke waarde van die voorspeller een regel als volgt; \
1: Tel hoe vaak elke waarde van doel (klasse) verschijnt. \
2: Zoek de meest voorkomende les. \
3: Laat de regel die klasse toewijzen aan deze waarde van de voorspeller. \
4: Bereken de totale fout van de regels van elke voorspeller. \
5: Kies de voorspeller met de kleinste totale fout.
OneR.information.example = Voorbeeld: De beste voorspeller vinden met de kleinste totale fout met behulp van het OneR-algoritme op basis van gerelateerde frequentietabellen.
OneR.information.modelEvaluation = Gewoon, de totale fout berekend uit de frequentietabellen is de maat van \
  elke voorspellerbijdrage. Een lage totale fout betekent een hogere bijdrage aan de voorspelbaarheid van het model.
NaiveBayes.information=De naïeve Bayesiaanse classificatie is gebaseerd op de stelling van Bayes met de \
  onafhankelijkheidsveronderstellingen tussen voorspellers. Een Naïef Bayesiaans model is eenvoudig te bouwen, zonder ingewikkelde \
  iteratieve parameterschatting waardoor het bijzonder nuttig is voor zeer grote datasets. Ondanks zijn eenvoud, \
  de Naïeve Bayesiaanse classifier doet het vaak verrassend goed en wordt veel gebruikt omdat het vaak beter presteert dan meer geavanceerde \
  classificatie methoden. De stelling van Bayes biedt een manier om de posterieure kans te berekenen, \
  P(c|x), van P(c), P(x) en P(x|c). Naïeve Bayes-classificator gaat ervan uit dat het effect van de waarde van een voorspeller (x) \
  op een gegeven klasse (c) is onafhankelijk \
  van de waarden van andere voorspellers. Deze aanname wordt klassenvoorwaardelijke onafhankelijkheid genoemd.
NaiveBayes.information.example = In het ZeroR-model is er geen voorspeller, in het OneR-model proberen we de enige beste voorspeller te vinden, naïef Bayesiaans omvat alle voorspellers met behulp van de regel van Bayes en de onafhankelijkheidsaannames tussen voorspellers.
NaiveBayes.information.modelEvaluation = De posterieure kans kan worden berekend door eerst \
  het construeren van een frequentietabel voor elk attribuut tegen het doel. \
  Transformeer vervolgens de frequentietabellen naar waarschijnlijkheidstabellen en gebruik tenslotte de Naïeve Bayesiaanse \
  vergelijking om de posterieure kans voor elke klasse te berekenen. \
  De klasse met de hoogste posterieure waarschijnlijkheid is de uitkomst van de voorspelling.
IBK.information=K naaste buren is een eenvoudig algoritme dat alle beschikbare gevallen opslaat en nieuwe gevallen classificeert op basis van een overeenkomstmaat (bijv. afstandsfuncties). \
  In de statistieken is het k-nearest-algoritme (k-NN) een niet-parametrische classificatie\
  \ methode voor het eerst ontwikkeld door Evelyn Fix en Joseph Hodges in 1951, en later uitgebreid door Thomas Cover. Het wordt gebruikt voor classificatie \
  en regressie. In beide gevallen bestaat de invoer uit de k dichtstbijzijnde trainingsvoorbeelden in de dataset. De uitvoer hangt af van of k-NN is\
  \ gebruikt voor classificatie of regressie:\
In k-NN-classificatie is de uitvoer een klasselidmaatschap. Een object wordt geclassificeerd door meerdere stemmen van zijn buren,\
  \ waarbij het object wordt toegewezen aan de klasse die het meest voorkomt onder zijn k naaste buren (k is een positief geheel getal, meestal klein). \
  Een geval wordt geclassificeerd door een meerderheid van stemmen van zijn buren, waarbij het geval wordt toegewezen aan de klasse die het meest voorkomt onder zijn K naaste buren, gemeten met een afstandsfunctie. \
  Als K = 1, dan wordt het geval eenvoudig toegewezen aan de klasse van zijn naaste buur.
IBK.information.modelEvaluation= Een groot nadeel bij het direct uit de trainingsset berekenen van afstandsmaten is in het geval dat variabelen verschillende \
  meetschalen of er is een mengsel van numerieke en categorische variabelen. \
  Als de ene variabele bijvoorbeeld is gebaseerd op het jaarinkomen in dollars en de andere op leeftijd \
  in jaren heeft het inkomen dan een veel grotere invloed op de berekende afstand. Een oplossing is om de trainingsset te standaardiseren zoals hieronder weergegeven.


J48.information=J4.8 is Weka-implementatie van beslissingsboomalgoritme C4.5. Het is eigenlijk gebaseerd op het ID3-algoritme door \
  functies zoals snoeien toe te voegen en numerieke attribuutwaarden toe te staan. C4.5 is een algoritme dat wordt gebruikt om een \
  beslissingsboom te genereren die is ontwikkeld door Ross Quinlan. C4.5 is een uitbreiding van het eerdere ID3-algoritme van Quinlan. \
  De beslisbomen die door C4.5 worden gegenereerd, kunnen worden gebruikt voor classificatie en om deze reden wordt C4.5 vaak een statistische \
  classificatie genoemd. In 2011 beschreven auteurs van de Weka machine learning-software het C4.5-algoritme als "een baanbrekend beslissingsboomprogramma \
  dat waarschijnlijk het machine learning-werkpaard is dat tot nu toe in de praktijk het meest wordt gebruikt. De beslissingsboom bouwt classificatie- of \
  regressiemodellen op in de vorm van een boomstructuur. Het splitst een dataset op in kleinere en kleinere subsets, \
  terwijl tegelijkertijd een bijbehorende beslisboom stapsgewijs wordt ontwikkeld. Het eindresultaat is een boom met \
  beslissingsknopen en bladknopen. Een beslissingsknooppunt (bijv. Outlook) heeft twee of meer takken (bijv. Sunny, Bewolkt en Rainy). \
  Bladknooppunt (bijv. Spelen) vertegenwoordigt een classificatie of beslissing. Het bovenste beslissingsknooppunt in een boom die overeenkomt \
  met de beste voorspeller, het wortelknooppunt. Beslisbomen kunnen zowel categorische als numerieke gegevens verwerken. \
  Het bovenste beslissingsknooppunt in een boom die overeenkomt met de beste voorspeller, het wortelknooppunt. \
  Beslisbomen kunnen zowel categorische als numerieke gegevens verwerken. Het bovenste beslissingsknooppunt in een boom \
  die overeenkomt met de beste voorspeller, het wortelknooppunt. Beslisbomen kunnen zowel categorische als numerieke gegevens verwerken.
J48.information.example = Het kernalgoritme voor het bouwen van beslissingsbomen, ID3 genoemd door J.R. Quinlan, dat gebruik maakt van een top-down, \
  hebzuchtig zoeken door de ruimte van mogelijke takken zonder backtracking. ID3 gebruikt Entropy en Information Gain om een\
  beslissingsboom. In het ZeroR-model is er geen voorspeller, in het OneR-model proberen we de beste voorspeller te vinden, naïeve Bayesiaanse omvat \
  alle voorspellers die de regel van Bayes gebruiken en de onafhankelijkheidsaannames tussen voorspellers, maar de beslissingsboom omvat alle voorspellers met afhankelijkheidsaannames tussen voorspellers.
J48.information.modelEvaluation = Een beslissingsboom wordt top-down opgebouwd vanuit een hoofdknooppunt en omvat het partitioneren van de gegevens in subsets die instanties met vergelijkbare waarden (homogeen) bevatten. ID3-algoritme gebruikt entropie om de homogeniteit van een monster te berekenen. \
  Als het monster volledig homogeen is, is de entropie nul en als het monster gelijk verdeeld is, heeft het een entropie van één.

#NAVBAR FRAGMENT
navbar.about=Over de applicatie
navbar.contact=Contact
navbar.home=Home

#PARAMETER FRAGMENT
max.batchsize=Maximale Batch grootte
debug.par=Debug
doNotCheck.par=Check geen capaciteiten
num.decimal.par=Aantal decimalen
minBucketSize.par=Minimale bucket grootte
confidenceFactor.par=Vertouwens factor
minNumObj.par=Minimale aantal objecten
numFold.par=Aantal vouwingen
laPlace.par=la Place schatting
pruned.par=Gesnoeid
knn.par=K-dichtstbijzijnde waarde
crossValidate.par=Kruisvalideer
nnsearchAlgorithm.par=Nearest neighbour\
zoek algoritme
true=Ja
false=Nee

#PARAMETER EXPLAINATION
batchsize.explaination=Het gewenste aantal instanties\n\
  dat moet worden verwerkt.
debug.explaination=Of de classifier extra informatie laat zien.
donotcheckcap.explaination=Of classificatie mogelijkheden\n\
  niet gecontroleerd worden vordat\n\
  de classifier wordt gebouwd.
numdecimals.explaination=Het aantal decialen dat wordt\n\
  gebruikt voor de uitkomst.
minbucket.explaination=De minimale bucket grootte die\n\
  wordt gebruikt bij discretiseren\n\
  van numerieke kenmerken in groepen.
confactor.explaination=Wordt gebruikt bij pruining.\n\
  Des te kleiner het getal, des\n\
  te meer pruining plaats vindt.
minnumobj.explaination=Minimale aantal instanties\n\
  per tak van de boom.
numfolds.explaination=Hoeveel data wordt gebruikt\n\
  om de boom te pruinen.
laplace.explaination=
pruned.explaination=Of de gemaakte boom\n\
  verkleind wordt.
knn.explaination=Het aantal dichtbijzijnde\n\
  punten dat wordt gebruikt.
crossval.explaination=Of hold-one-out kruisvalidatie\n\
  wordt toegepast om de beste\n\
  K-waarde te selecteren
nnsearch.explaination=Het NN zoek algorithme

#FILE SELECT FORM
form.select=Selecteer het demo bestand:
form.upload=Of upload een eigen bestand:
form.choose=Kies een bestand
form.file.select=Selecteer...

#EXPLORER TAB
explorer=Ontdeker
explorer.attributes=Atributen
explorer.select=Selecteer atribuut

#CLASSIFIER TAB
classify=Classificeer
form.classifier.select=Selecteer de classificatie:

#RESULTS TAB
results=Resultaten
results.correct=Goed geclassificeerde instanties            
results.perc.correct=Percentage goed geclassificeerde instanties 
results.incorrect=Fout geclassificeerde instanties            
results.perc.incorrect=Percentage fout geclassificeerde instanties 
results.kappa=Kappa statistiek                            
results.mean.abs=Gemiddelde afwijking                        
results.mean.squared=Gemiddelde kwadratische afwijking           
results.relative.abs=Relatieve afwijking                         
results.relative.squared=Relatieve kwadratische afwijking            
results.instances.num=Totaal aantal instanties                    
results.message=Er zijn nog geen resultaten beschrikbaar.

#HISTORY TAB
history=Geschiedenis
history.time=Tijd/Datum
history.usedFile=Gebuikt bestand
history.uploaded=Geuploade bestand(en)
history.explorer=Geschiedenis ontdekker
history.classifier=Geschiedenis classificatie

#RESULTS EXPLAINATION
numInstances.explaination=Het aantal instanties wat gebruikt wordt voor het bouwen van\
het model
correct.explaination=Het aantal correct geclassificeerde instanties
pctCorrect.explaination=Het percentage van het totaal aantal instanties dat correct\
geclassificeerd is.
incorrect.explaination=Het aantal incorrect geclassificeerde instanties
pctIncorrect.explaination=Het percentage van het totaal aantal instanties dat incorrect\
geclassificeerd is.
kappa.explaination=De kappa-statistiek is een manier om de betrouwbaarheid te bepalen\
van een model. Het wordt uitgedrukt als een waarde tussen 0 en 1. \
Een waarde van 0 betekent dat het model niet betrouwbaar is en \
de resultaten hoogstwaarschijnlijk toeval zijn. Een waarde van\
 1 betekent dat het zeer betrouwbaar is en waarschijnlijk \
 niet toevallig is.
meanAbsoluteError=In statistieken is de gemiddelde absolute fout een maatstaf voor het verschil tussen twee continue variabelen. Neem aan dat X en Y variabelen zijn van gepaarde waarnemingen die hetzelfde fenomeen uitdrukken. Voorbeelden van Y versus X omvatten vergelijkingen van voorspelde versus waargenomen, daaropvolgende tijd versus initiële tijd, en één meettechniek versus een alternatieve meettechniek.
rootMeanSquaredError=De kwadratische afwijking of kwadratisch gemiddelde fout is een veelgebruikte maatstaf voor de verschillen tussen waarden die worden voorspeld door een model of een schatter en de waargenomen waarden. De RMSD vertegenwoordigt de vierkantswortel van het tweede monstermoment van de verschillen tussen voorspelde waarden en waargenomen waarden of het kwadratische gemiddelde van deze verschillen.
#TODO: nog implementeren
relativeAbsoluteError=Nog niet geimplementeerd
rootRelativeSquaredError=Nog niet geimplementeerd

#ERROR PAGE
404.title=Oeps, deze  pagina bestaat niet
404.subtitle=De verzochte pagina lijkt niet te bestaan.
500.title=Oeps, er is iets misgegaan
500.subtitle=Er is helaas iets mis gegaan in onze server.
400.title=Oh nee! Een defect verzoek.
400.subtitle=Er is iets mis gegaan met het communiceren van je input<br>\
naar de server. Zorg ervoor dat je formulieren invult voor<br>\
dat je op de 'verstuur' knop drukt.

#TODO: worden niet gebruikt, nog verwijderen
infopage.basicinfoweka="Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
infopage.title=Weka Algortihms
student.email=Studenten email
student.names=Studenten
results.title=Resultaten
button.return=Ga terug
HistoryList=Geschiedenis lijst
explorer.title=Weka Werkbank
explorer.subtitle=Start met je dataset bekijken en het maken van een classifier
error.title=Oeps, er is een fout opgetreden
error.subtitle=Er is iets misgegaan maar we weten niet precies wat

