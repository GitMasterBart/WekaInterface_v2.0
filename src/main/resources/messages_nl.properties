button=Ga naar de pagina
explorer.info=Ontdek hoe weka werkt.
information=Wat is machine learning?
home.subtitle=Ontdek machine learning en pas het toe met onze makkelijk te gebruiken webapplicatie.
home.title=Klessify
infopage.basicinfoweka="Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
infopage.title=Weka Algortihms
navbar.about=Over de applicatie
navbar.contact=Contact
navbar.home=Home
student.email=Studenten email
student.names=Studenten
contact.title=Contact
contact.subtitle=Dit is het team achter onze Weka Interface.
marijke.info=De Back End ontwikkelaar, verantwoordelijk voor alle bewegende onderdelen achter de schermen wat communiceert met de HTML pagina's.
bart.info=Onze gitmaster, verantwoordelijk voor het versie management \
zodat wanneer je samen aan een code project werkt, alles\
soepel verloopt.
jelle.info=De Front End ontwikkelaar, maakt de HTML paginas die je ziet\
 en voegt hier fucntionaliteit aan toe.
michiel.info=De project begeleider, geeft advies over het oplossen van problemen\
 en voorziet het team van informatie bronnen.

ZeroR.information=De eenvoudigste van de op regels gebaseerde classificaties is de meerderheidsclassificatie, genaamd 0-R of\
ZeroR in Weka. De classifier 0-R (nulregel) kijkt naar het doelkenmerk en de mogelijke ervan\
waarden. Het zal altijd de waarde uitvoeren die het meest wordt gevonden voor het doelkenmerk in de \
gegeven dataset. 0-R zoals de naam doet vermoeden; het bevat geen enkele regel die werkt op het niet-doel\
attributen. Dus meer specifiek voorspelt het het gemiddelde (voor een numeriek type doelkenmerk) of de modus\
(voor een nominaal type attribuut).
ZeroR.information.example = Voorbeeld: "Play Golf = Yes" is het ZeroR-model voor de dataset met een nauwkeurigheid van 0,64 is 9 ja en 5 nee .
ZeroR.information.modelEvaluation = ZeroR voorspelt alleen de meerderheidsklasse correct. Zoals eerder vermeld, \
ZeroR is alleen nuttig voor het bepalen van een basisprestatie voor andere classificatiemethoden.
OneR.information=OneR, een afkorting voor "One Rule", is een eenvoudig, maar nauwkeurig classificatie-algoritme dat één regel genereert \
  voor elke voorspeller in de gegevens, selecteert vervolgens de regel met de kleinste totale fout als zijn "ene regel". \
  Om een regel voor een voorspeller te maken, construeren we een frequentietabel voor elke voorspeller tegen het doel. \
  Het is aangetoond dat OneR regels slechts iets minder nauwkeurig produceert dan state-of-the-art classificatie-algoritmen \
  terwijl ze regels produceren die voor mensen eenvoudig te interpreteren zijn.\
  Voor elke voorspeller,\
  Maak voor elke waarde van die voorspeller een regel als volgt; \
1: Tel hoe vaak elke waarde van doel (klasse) verschijnt. \
2: Zoek de meest voorkomende les. \
3: Laat de regel die klasse toewijzen aan deze waarde van de voorspeller. \
4: Bereken de totale fout van de regels van elke voorspeller. \
5: Kies de voorspeller met de kleinste totale fout.
OneR.information.example = Voorbeeld: De beste voorspeller vinden met de kleinste totale fout met behulp van het OneR-algoritme op basis van gerelateerde frequentietabellen.
OneR.information.modelEvaluation = Gewoon, de totale fout berekend uit de frequentietabellen is de maat van \
  elke voorspellerbijdrage. Een lage totale fout betekent een hogere bijdrage aan de voorspelbaarheid van het model.
NaiveBayes.information=De naïeve Bayesiaanse classificatie is gebaseerd op de stelling van Bayes met de \
  onafhankelijkheidsveronderstellingen tussen voorspellers. Een Naïef Bayesiaans model is eenvoudig te bouwen, zonder ingewikkelde \
  iteratieve parameterschatting waardoor het bijzonder nuttig is voor zeer grote datasets. Ondanks zijn eenvoud, \
  de Naïeve Bayesiaanse classifier doet het vaak verrassend goed en wordt veel gebruikt omdat het vaak beter presteert dan meer geavanceerde \
  classificatie methoden. De stelling van Bayes biedt een manier om de posterieure kans te berekenen, \
  P(c|x), van P(c), P(x) en P(x|c). Naïeve Bayes-classificator gaat ervan uit dat het effect van de waarde van een voorspeller (x) \
  op een gegeven klasse (c) is onafhankelijk \
  van de waarden van andere voorspellers. Deze aanname wordt klassenvoorwaardelijke onafhankelijkheid genoemd.
NaiveBayes.information.example = In het ZeroR-model is er geen voorspeller, in het OneR-model proberen we de enige beste voorspeller te vinden, naïef Bayesiaans omvat alle voorspellers met behulp van de regel van Bayes en de onafhankelijkheidsaannames tussen voorspellers.
NaiveBayes.information.modelEvaluation = De posterieure kans kan worden berekend door eerst \
  het construeren van een frequentietabel voor elk attribuut tegen het doel. \
  Transformeer vervolgens de frequentietabellen naar waarschijnlijkheidstabellen en gebruik tenslotte de Naïeve Bayesiaanse \
  vergelijking om de posterieure kans voor elke klasse te berekenen. \
  De klasse met de hoogste posterieure waarschijnlijkheid is de uitkomst van de voorspelling.

IBK.information=K naaste buren is een eenvoudig algoritme dat alle beschikbare gevallen opslaat en nieuwe gevallen classificeert op basis van een overeenkomstmaat (bijv. afstandsfuncties). \
  In de statistieken is het k-nearest-algoritme (k-NN) een niet-parametrische classificatie\
  \ methode voor het eerst ontwikkeld door Evelyn Fix en Joseph Hodges in 1951, en later uitgebreid door Thomas Cover. Het wordt gebruikt voor classificatie \
  en regressie. In beide gevallen bestaat de invoer uit de k dichtstbijzijnde trainingsvoorbeelden in de dataset. De uitvoer hangt af van of k-NN is\
  \ gebruikt voor classificatie of regressie:\
In k-NN-classificatie is de uitvoer een klasselidmaatschap. Een object wordt geclassificeerd door meerdere stemmen van zijn buren,\
  \ waarbij het object wordt toegewezen aan de klasse die het meest voorkomt onder zijn k naaste buren (k is een positief geheel getal, meestal klein). \
  Een geval wordt geclassificeerd door een meerderheid van stemmen van zijn buren, waarbij het geval wordt toegewezen aan de klasse die het meest voorkomt onder zijn K naaste buren, gemeten met een afstandsfunctie. \
  Als K = 1, dan wordt het geval eenvoudig toegewezen aan de klasse van zijn naaste buur.
IBK.information.modelEvaluation= Een groot nadeel bij het direct uit de trainingsset berekenen van afstandsmaten is in het geval dat variabelen verschillende \
  meetschalen of er is een mengsel van numerieke en categorische variabelen. \
  Als de ene variabele bijvoorbeeld is gebaseerd op het jaarinkomen in dollars en de andere op leeftijd \
  in jaren heeft het inkomen dan een veel grotere invloed op de berekende afstand. Een oplossing is om de trainingsset te standaardiseren zoals hieronder weergegeven.

J48.information=J4.8 is Weka implementation of decision tree algorithm C4.5. It is actually built upon ID3\
algorithm by adding features like pruning as well as allowing numeric attribute values.\
C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. \
  C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for \
  classification, and for this reason, C4.5 is often referred to as a statistical classifier. \
  In 2011, authors of the Weka machine learning software described the C4.5 algorithm as "a landmark decision tree program \
  that is probably the machine learning workhorse most widely used in practice to date. Decision tree builds classification or regression models in the form of a tree structure. \
  It breaks down a dataset into smaller and smaller \
  subsets while at the same time an associated decision tree is incrementally developed. \
  The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy). \
  Leaf node (e.g., Play) represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. \
  Decision trees can handle both categorical and numerical data.

J48.information.example = Het kernalgoritme voor het bouwen van beslissingsbomen, ID3 genoemd door J.R. Quinlan, dat gebruik maakt van een top-down, \
  hebzuchtig zoeken door de ruimte van mogelijke takken zonder backtracking. ID3 gebruikt Entropy en Information Gain om een ​​\
  beslissingsboom. In het ZeroR-model is er geen voorspeller, in het OneR-model proberen we de beste voorspeller te vinden, naïeve Bayesiaanse omvat \
  alle voorspellers die de regel van Bayes gebruiken en de onafhankelijkheidsaannames tussen voorspellers, maar de beslissingsboom omvat alle voorspellers met afhankelijkheidsaannames tussen voorspellers.

J48.information.modelEvaluation = Een beslissingsboom wordt top-down opgebouwd vanuit een hoofdknooppunt en omvat het partitioneren van de gegevens in subsets die instanties met vergelijkbare waarden (homogeen) bevatten. ID3-algoritme gebruikt entropie om de homogeniteit van een monster te berekenen. \
  Als het monster volledig homogeen is, is de entropie nul en als het monster gelijk verdeeld is, heeft het een entropie van één.



form.select=Selecteer het demo bestand: 
form.upload=Of upload een eigen bestand:
form.choose=Kies een bestand
form.classifier.select=Selecteer de classifier:
true=Ja
false=Nee
results.title=Resultaten
max.batchsize=Maximale Batch grootte
debug.par=Debug
doNotCheck.par=Check geen capaciteiten
num.decimal.par=Aantal decimalen
minBucketSize.par=Minimale bucket grootte
confidenceFactor.par=Vertouwens factor
minNumObj.par=Minimale aantal objecten
numFold.par=Aantal vouwingen
laPlace.par=la Place schatting
pruned.par=Gesnoeid
knn.par=K-dichtstbijzijnde waarde
crossValidate.par=Kruisvalideer
nnsearchAlgorithm.par=Nearest neighbour\
zoek algoritme
form.file.select=Selecteer...
wekaAlgorithms.title=Weka Algorithme
button.return=Ga terug
HistoryList=Geschiedenis lijst
results.correct=Goed geclassificeerde instanties            
results.perc.correct=Percentage goed geclassificeerde instanties 
results.incorrect=Fout geclassificeerde instanties            
results.perc.incorrect=Percentage fout geclassificeerde instanties 
results.kappa=Kappa statistiek                            
results.mean.abs=Gemiddelde afwijking                        
results.mean.squared=Gemiddelde kwadratische afwijking           
results.relative.abs=Relatieve afwijking                         
results.relative.squared=Relatieve kwadratische afwijking            
results.instances.num=Totaal aantal instanties                    
results.message=Er zijn nog geen resultaten beschrikbaar.
about.title=Over
about.subtitle=Wat is het nut van deze web applicatie?

explorer.title=Weka Werkbank
explorer.subtitle=Start met je dataset bekijken en het maken van een classifier
404.title=Oeps, deze  pagina bestaat niet
404.subtitle=De verzochte pagina lijkt niet te bestaan.
500.title=Oeps, er is iets misgegaan
500.subtitle=Er is helaas iets mis gegaan in onze server.
400.title=Oh nee! Een defect verzoek.
400.subtitle=Er is iets mis gegaan met het communiceren van je input<br>\
naar de server. Zorg ervoor dat je formulieren invult voor<br>\
dat je op de 'verstuur' knop drukt.
information.title=Machine Learning
information.subtitle=Merriam Webster definieert machine learning als volgt:<br>\
<i>"De term machine learning (afgekort ML) verwijst naar het \
vermogen van een machine om zijn eigen prestaties te \
verbeteren. Het doet dit door een statistisch model te \
gebruiken om beslissingen te nemen en het resultaat van \
elke nieuwe proef in dat model op te nemen. In wezen is de \
machine geprogrammeerd om met 'trial and error' te leren."</i>
WekaAlgorithms.subtitle=Open het dropdown menu om meer te leren over de verschillende\
machine learning algoritmen die wij geimplementeerd hebben in\
onze web applicatie.

about.text=Als beginner, kan leren over machine learning ongelooflijk \
uitdagend zijn.Het interpreteren van de resultaten van een \
 model vereist aanzienlijke kennis over algoritmen, algoritmen \
parameters en wiskunde. Daarnaast zijn de middelen die \
beschikbaar tijdens deze steile leercurve niet erg \
beginnersvriendelijk. De weka-software, hoe krachtig deze \
ook is, is moeilijk te gebruiken als beginner vanwege \
het niet-intuïtieve ontwerp en het gebrek aan informatie.<br> \
<br> \
Om deze beginnerslast te verlichten, hebben we deze web applicatie\
gemaakt om een gemakkelijke workflow te \
bieden in combinatie met toegang tot \
informatie die nodig is om de \
resultaten van je modellen en analyse te interpreteren.
numInstances.explaination=Het aantal instanties wat gebruikt wordt voor het bouwen van\
het model
correct.explaination=Het aantal correct geclassificeerde instanties
pctCorrect.explaination=Het percentage van het totaal aantal instanties dat correct\
geclassificeerd is.
incorrect.explaination=Het aantal incorrect geclassificeerde instanties
pctIncorrect.explaination=Het percentage van het totaal aantal instanties dat incorrect\
geclassificeerd is.
kappa.explaination=De kappa-statistiek is een manier om de betrouwbaarheid te bepalen\
van een model. Het wordt uitgedrukt als een waarde tussen 0 en 1. \
Een waarde van 0 betekent dat het model niet betrouwbaar is en \
de resultaten hoogstwaarschijnlijk toeval zijn. Een waarde van\
 1 betekent dat het zeer betrouwbaar is en waarschijnlijk \
 niet toevallig is.
meanAbsoluteError=In statistieken is de gemiddelde absolute fout een maatstaf voor het verschil tussen twee continue variabelen. Neem aan dat X en Y variabelen zijn van gepaarde waarnemingen die hetzelfde fenomeen uitdrukken. Voorbeelden van Y versus X omvatten vergelijkingen van voorspelde versus waargenomen, daaropvolgende tijd versus initiële tijd, en één meettechniek versus een alternatieve meettechniek.
rootMeanSquaredError=De kwadratische afwijking of kwadratisch gemiddelde fout is een veelgebruikte maatstaf voor de verschillen tussen waarden die worden voorspeld door een model of een schatter en de waargenomen waarden. De RMSD vertegenwoordigt de vierkantswortel van het tweede monstermoment van de verschillen tussen voorspelde waarden en waargenomen waarden of het kwadratische gemiddelde van deze verschillen.
relativeAbsoluteError=Nog niet geimplementeerd
rootRelativeSquaredError=Nog niet geimplementeerd
marijke.name=Marijke Eggink
jelle.name=Jelle Becirspahic
michiel.name=Michiel Noback
bart.name=Bart Engels
jelle.email=j.d.becirspahic@st.hanze.nl
marijke.email=m.eggink@st.hanze.nl
bart.email=b.engels@st.hanze.nl
michiel.email=m.a.noback@pl.hanze.nl
